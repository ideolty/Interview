# 综述

大量参考刘超老师的专栏《趣谈协议》

8个位(8bit)组成一个字节(1Byte)

cwnd(congestion window)拥塞窗口

rwnd(receive window)接收窗口

RTT(Round-Trip Time)往返时延

ack(Acknowledgment)应答

MSL(Maximum Segment Lifetime)报文最长存活时间



# 7层模型

参考:

[OSI七层模型与TCP/IP五层模型](https://www.cnblogs.com/qishui/p/5428938.html)



按照不同的标准可以分为7层模型与5层模型

![img](截图/计算机网络/OSI与TCP:IP模型对比图.png)

其中主要的协议

![img](截图/计算机网络/常见网络协议.png)



主要需要重点掌握的包括以下部分。

![image-20210210125307182](截图/计算机网络/网络协议模型.png)



# 物理层

> #### 集线器(hub)

它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。



# 数据链路层

也即 MAC 层要解决的问题。**MAC**的全称是**Medium Access Control**，即**媒体访问控制。**

主要解决

1. 这个包是发给谁的？谁应该接收？
2. 大家都在发，会不会产生混乱？有没有谁先发、谁后发的规则？（**多路访问**的堵车问题）
3. 如果发送的时候出现了错误，怎么办？



> #### 多路访问方式

- 方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作**信道划分；**
- 方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作**轮流协议；**
- 方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作**随机接入协议。**著名的以太网，用的就是这个方式。



> #### 发给谁，谁接收？

物理地址，叫作**链路层地址。**但是因为第二层主要解决媒体接入控制的问题，所以它常被称为**MAC 地址**。

第二层的网络包格式如下图

![index](截图/计算机网络/数据帧结构.jpg)

![frame](截图/计算机网络/Mac帧结构图.png)



- **前同步码**：第一个字段是**7个字节**的前同步码，1和0交替，作用是用来使接收端的适配器在接收MAC帧时能够迅速调整时钟频率，使它和发送端的频率相同。

- **帧开始定界符**：第二个字段是**1个字节**的帧开始定界符，前六位1和0交替，最后的两个连续1表示告诉接收端适配器：“帧信息要来了，你准备接收把。

  

对于以太网，第二层的最开始，就是目标的 MAC 地址和源的 MAC 地址。接下来是**类型**，大部分的类型是 IP 数据包，然后 IP 里面包含 TCP、UDP，以及 HTTP 等，这都是里层封装的事情。

有了这个目标 MAC 地址，数据包在链路上广播，MAC 的网卡才能发现，这个包是给它的。MAC 的网卡把包收进来，然后打开 IP 包，发现 IP 地址也是自己的，再打开 TCP 包，发现端口是自己，也就是 80，而 nginx 就是监听 80。



## 数据校验CRC

对于以太网，第二层的最后面是**CRC**，也就是**循环冗余检测**。通过 XOR 异或（不同为1，同或相同为1，只记为1的情况就不会记乱）的算法，来计算整个包是否在发送的过程中出现了错误。

原始文字描述见下方的引用，现来个例子：现假设选择的CRC生成多项式为$G(x) = x^4 + x^3 + 1$，要求出二进制序列10110011的CRC校验码。（这里注意了，由于MAC帧校验位只有4位，所以多项式最高次为5，也就是二进制校验串的长度为5）

下面是具体的计算过程：

1. 首先把生成多项式转换成二进制数，由$G(x) = x^4 + x^3 + 1$可以知道：它的二进制比特串为11001。

2. 因为生成多项式的位数为5，所以CRC校验码的位数为4（校验码的位数比生成多项式的位数少1）。因为原数据帧10110011，在它后面再加4个0，得到101100110000，然后把这个数以“模2除法”方式除以生成多项式，得到的余数（即CRC码）为0100，如图所示。注意参考前面介绍的“模2除法”运算法则。

   <img src="截图/计算机网络/CRC校验码计算示例.png" alt="image-20210226135301192" style="zoom: 85%;" />

3. 把上步计算得到的CRC校验0100替换原始帧101100110000后面的四个“0”，得到新帧101100110100。再把这个新帧发送到接收端。

4. 当以上新帧到达接收端后，接收端会把这个新帧再用上面选定的除数11001以“模2除法”方式去除，验证余数是否为0，如果为0，则证明该帧数据在传输过程中没有出现差错，否则出现了差错。



> [CRC校验](https://mp.weixin.qq.com/s/RNHLZGPD9Ysbxb1FNDn6EA?)
>
> [以太网帧结构以及CRC校验](https://blog.csdn.net/a1414345/article/details/72781130/)
>
> 以及在B站找到的一个非常好的视频讲解，非常直观的给出了手算与位运算的情况
>
> [[CRC校验]手算与直观演示](https://www.bilibili.com/video/BV1V4411Z7VA)



------


> #### 交换机

Hub 是广播的，不管某个接口是否需要，所有的 Bit 都会被发送出去，然后让主机来判断是不是需要。这种方式路上的车少就没问题，车一多，产生冲突的概率就提高了。而且把不需要的包转发过去，纯属浪费。交换机把MAC 头拿下来，检查一下目标 MAC 地址，然后根据策略转发。

**工作原理**

一台 MAC1 电脑将一个包发送给另一台 MAC2 电脑，当这个包到达交换机的时候，一开始交换机也不知道 MAC2  的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。这个时候交换机会记住，MAC1 是来自一个明确的口。以后有包的目的地址是 MAC1 的，直接发送到这个口就可以了。

当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的 IP 地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为**转发表**，是有一个过期时间的。



## ARP 协议

全称是Address Resolution Protocol，已知 IP 地址，求 MAC 地址的协议。

在一个局域网里面，当知道了 IP 地址，不知道 MAC 怎么办呢？广而告之，发送一个广播包，谁是这个 IP 谁来回答。为了避免每次都用 ARP 请求，机器本地也会进行 ARP 缓存。当然机器会不断地上线下线，IP 也可能会变，所以 ARP 的 MAC 地址缓存过一段时间就会过期。

整个流程如下：

![index](截图/计算机网络/ARP请求流程图.jpg)



具体询问和回答的报文就像下面这样：

![index](截图/计算机网络/ARP报文.jpg)

当发送的时候，目标MAC由于还不知道，全部为0.



## STP 协议

> 第6讲 | 交换机与VLAN：办公室太复杂，我要回学校

网络拓扑结构相关内容。



> #### 如何解决常见的环路问题？

当交换机的数目越来越多的时候，会遭遇环路问题，让网络包迷路，这就需要使用 STP 协议，将有环路的图变成没有环路的树，从而解决环路问题。





> #### 如何解决广播问题和安全问题？

有两种分的方法，一个是**物理隔离**。单独的交换机，配置单独的子网。另外一种方式是**虚拟隔离**，就是用我们常说的**VLAN**，或者叫**虚拟局域网**。

使用 VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？

![img](截图/计算机网络/VLAN协议模型.jpeg)

我们只需要在原来的二层的头上加一个 TAG，里面有一个 VLAN ID，一共 12 位，12 位可以划分  4096 个 VLAN。这样是不是还不够啊。现在的情况证明，目前云计算厂商里面绝对不止 4096 个用户。当然每个用户需要一个 VLAN  了啊，怎么办呢？//todo 

当这个交换机把二层的头取下来的时候，就能够识别这个 VLAN ID。这样只有相同 VLAN 的包，才会互相转发，不同 VLAN 的包，是看不到的。这样广播问题和安全问题就都能够解决了。

交换机之间怎么连接呢？将两个交换机连接起来的口应该设置成什么 VLAN 呢？对于支持 VLAN 的交换机，有一种口叫作**Trunk 口**。它可以转发属于任何 VLAN 的口。交换机之间可以通过这种口相互连接。



# 网络层

## ICMP 协议

> 第7讲 | ICMP与ping：投石问路的侦察兵      

**ICMP**全称**Internet Control Message Protocol**，就是**互联网控制报文协议**。这里面的关键词是“控制”。ping 是基于 ICMP 协议工作的。ICMP 报文是封装在 IP 包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。

![img](截图/计算机网络/ICMP协议格式.jpeg)

ICMP 报文有很多的类型，不同的类型有不同的代码。**最常用的类型是主动请求为 8，主动请求的应答为 0**。

> #### 查询报文类型

常用的**ping 就是查询报文，是一种主动请求，并且获得主动应答的 ICMP 协议。**对 ping 的主动请求，称为**ICMP ECHO REQUEST。**同理主动请求的回复，称为**ICMP ECHO REPLY**。

比起原生的 ICMP，这里面多了两个字段，一个是**标识符**。这个很好理解，你派出去两队侦查兵，一队是侦查战况的，一队是去查找水源的，要有个标识才能区分。另一个是**序号**，你派出去的侦查兵，都要编个号。在选项数据中，ping 还会存放发送请求的时间值，来计算往返时间，说明路程的长短。



> #### 差错报文类型

这种是异常情况发起的，来报告发生了不好的事情，对应 ICMP 的**差错报文类型**。

几个 ICMP 差错报文的例子：**终点不可达为 3，源抑制为 4，超时为 11，重定向为 5**。

**第一种是终点不可达**。

结果没有送到。具体的原因在代码中表示就是，网络不可达代码为 0，主机不可达代码为 1，协议不可达代码为 2，端口不可达代码为 3，需要进行分片但设置了不分片位代码为 4。

**第二种是源站抑制**，也就是让源站放慢发送速度。

**第三种是时间超时**，也就是超过网络包的生存时间还是没到。

**第四种是路由重定向**，也就是让下次发给另一个路由器。

差错报文的结构相对复杂一些。除了前面还是 IP，ICMP 的前 8 字节不变，后面则跟上出错的那个 IP 包的 IP 头和 IP 正文的前 8 个字节。



**Traceroutem命令**

ping 使用查询报文，Traceroute 使用差错报文。

**Traceroute 的第一个作用就是故意设置特殊的 TTL，来追踪去往目的地时沿途经过的路由器**。

**Traceroute 还有一个作用是故意设置不分片，从而确定路径的 MTU。**







## IP协议

![index](截图/计算机网络/IP分类.jpg)

![index](截图/计算机网络/IP分类主机数.jpg)

上面这个表格，详细地展示了 A、B、C 三类地址所能包含的主机的数量。



> #### 无类型域间选路（CIDR）

由于C类地址实在太少，于是有了一个折中的方式叫作**无类型域间选路（Classless Inter-Domain Routing）**，简称**CIDR**。这种方式打破了原来设计的几类地址的做法，将 32 位的 IP 地址一分为二，前面是**网络号**，后面是**主机号**。从哪里分呢？你如果注意观察的话可以看到，10.100.122.2/24，这个 IP 地址中有一个斜杠，斜杠后面有个数字 24。这种地址表示形式，就是 CIDR。后面 24 的意思是，32 位中，前 24 位是网络号，后 8 位是主机号。

伴随着 CIDR 存在的，一个是**广播地址**，10.100.122.255。如果发送这个地址，所有 10.100.122 网络里面的机器都可以收到。另一个是**子网掩码**，255.255.255.0。

将子网掩码和 IP 地址进行 AND 计算。前面三个 255，转成二进制都是 1。1 和任何数值取  AND，都是原来数值，因而前三个数不变，为 10.100.122。后面一个 0，转换成二进制是 0，0 和任何数值取 AND，都是  0，因而最后一个数变为 0，合起来就是 10.100.122.0。这就是**网络号**。**将子网掩码和 IP 地址按位计算 AND，就可得到网络号。**



> #### 多播与组播的区别

一个多播地址需要的客户端就加入，**“多播”也可以称为“组播”**，在网络技术的应用并不是很多，网上视频会议、网上视频点播特别适合采用多播方式。因为如果采用单播方式，逐个节点传输，有多少个目标节点，就会有多少次传送过程，这种方式显然效率极低，是不可取的；如果采用不区分目标、全部发送的广播方式，虽然一次可以传送完数据，但是显然达不到区分特定数据接收对象的目的。采用多播方式，既可以实现一次传送所有目标节点的数据，也可以达到只对特定对象传送数据的目的。　　

IP网络的多播一般通过多播IP地址来实现。多播IP地址就是D类IP地址，即224.0.0.0至239.255.255.255之间的IP地址。




> #### 随便填一个ip会发送什么？
>
> 第04讲丨DHCP与PXE：IP是怎么来的，又是怎么没的？—— 《趣谈协议》

不会出现任何现象，就是包发不出去呗。

192.168.1.6 就在你这台机器的旁边，甚至是在同一个交换机上，而你把机器的地址设为了 16.158.23.6。在这台机器上，你企图去 ping192.168.1.6。

这个包它有自己的源 IP 地址 16.158.23.6，也有目标 IP 地址 192.168.1.6，但是包发不出去，这是因为 MAC 层还没填。自己的 MAC 地址自己知道，这个容易。但是目标 MAC 填什么呢？是不是填 192.168.1.6 这台机器的 MAC 地址呢？

当然不是。Linux 首先会判断，要去的这个地址和我是一个网段的吗，或者和我的一个网卡是同一网段的吗？只有是一个网段的，它才会发送 ARP 请求，获取 MAC 地址。如果发现不是呢？**Linux 默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。**

如果你配置了网关的话，Linux 会获取网关的 MAC 地址，然后将包发出去。对于 192.168.1.6 这台机器来讲，目标 IP 是它，但是无奈 MAC 地址不是它的，所以它的网卡是不会把包收进去的。

如果没有配置网关呢？那包压根就发不出去。如果将网关配置为 192.168.1.6 呢？不可能，Linux 不会让你配置成功的，因为网关要和当前的网络至少一个网卡是同一个网段的，怎么可能 16.158.23.6 的网关是 192.168.1.6 呢？



### IP头

> 第8讲 | 世界这么大，我想出网关：欧洲十国游与玄奘西行      

![index](截图/计算机网络/IP与MAC头.jpg)

在 MAC 头里面

- 先是目标 MAC 地址，然后是源 MAC 地址，然后有一个协议类型，用来说明里面是 IP 协议。

- IP  头里面的版本号，目前主流的还是 IPv4
- **IP包头长度（Header Length）**：长度4比特。这个字段的作用是为了描述IP包头的长度，因为在IP包头中有变长的可选部分。该部分占4个bit位，单位为32bit（4个字节），即本区域值= IP头部长度（单位为bit）/(8*4)，因此，一个IP包头的长度最长为“1111”，即15*4＝60个字节。IP包头最小长度为20字节。
- **服务类型 TOS** ：在第三节讲 ip addr 命令的时候讲过（代表了当前的包是高优先级的，还是低优先级的）
- **IP包总长（Total Length）**：长度16比特。 以字节为单位计算的IP包的长度 (包括头部和数据)，所以IP包最大长度65535字节。
- **标识（Identifier）**：长度16比特。该字段和Flags和Fragment Offest字段联合使用，对较大的上层数据包进行分段（fragment）操作。路由器将一个包拆分后，所有拆分开的小包被标记相同的值，以便目的端设备能够区分哪个包属于被拆分开的包的一部分。
- **标志（Flags）**：长度3比特。该字段第一位不使用。第二位是DF（Don't  Fragment）位，DF位设为1时表明路由器不能对该上层数据包分段。如果一个上层数据包无法在不分段的情况下进行转发，则路由器会丢弃该上层数据包并返回一个错误信息。第三位是MF（More Fragments）位，当路由器对一个上层数据包分段，则路由器会在除了最后一个分段的IP包的包头中将MF位设为1。
- **片偏移（Fragment Offset）**：长度13比特。表示该IP包在该组分片包中位置，接收端靠此来组装还原IP包。
- **生存时间（TTL）**：长度8比特。当IP包进行传送时，先会对该字段赋予某个特定的值。当IP包经过每一个沿途的路由器的时候，每个沿途的路由器会将IP包的TTL值减少1。如果TTL减少为0，则该IP包会被丢弃。这个字段可以防止由于路由环路而导致IP包在网络中不停被转发。
- 另外，还有 8 位标识协议。这里到了下一层的协议，也就是，是 TCP 还是 UDP。
- **头部校验（Header Checksum）**：长度16位。用来做IP头部的正确性检测，但不包含数据部分。 因为每个路由器要改变TTL的值,所以路由器会为每个通过的数据包重新计算这个值。
- 最重要的就是源 IP 和目标 IP。先是源  IP 地址，然后是目标 IP 地址。



> #### 检验和算法

1. 把IP数据包的校验和字段置为0； 　　
2. 把首部看成以16位为单位的数字组成，依次进行二进制求和（注意：求和时应将最高位的进位保存，所以加法应采用32位加法）； 　　
3. 将上述加法过程中产生的进位（最高位的进位）加到低16位（采用32位加法时，即为将高16位与低16位相加，之后还要把该次加法最高位产生的进位加到低16位） 　　
4. 将上述的和取反，即得到校验和。

例子：

> 例子:
>
>    IP头：
>
>    45 00   00 31
>
>    89 F5   00 00
>
>    6E 06   00 00（校验字段）
>
>    DE B7  45 5D   ->  222.183.69.93
>
>    C0 A8  00 DC   ->  192.168.0.220
>
>    计算： 
>
>    4500 + 0031 +89F5 + 0000 + 6e06+ 0000 + DEB7 + 455D + C0A8 + 00DC =3 22C4
>
>    0003 + 22C4 = 22C7
>
>    ~22C7 = DD38   -> 即为应填充的校验和
>
>    当接受到IP数据包时，要检查IP头是否正确，则对IP头进行检验，方法同上：
>
>    计算：
>
>    4500 + 0031 +89F5 + 0000 + 6E06+ DD38 + DEB7 + 455D + C0A8 + 00DC =3 FFFC
>
>    0003 + FFFC = FFFF
>
>    得到的结果是全一，正确。

[ip首部校验和计算](https://blog.csdn.net/gao1440156051/article/details/51210320)

------

在任何一台机器上，当要访问另一个 IP 地址的时候，都会先判断，这个目标 IP 地址，和当前机器的 IP 地址，是否在同一个网段。怎么判断同一个网段呢？需要 CIDR 和子网掩码，这个在第三节的时候也讲过了。

**如果是同一个网段**，那就没网关什么事情，直接将源地址和目标地址放入 IP 头中，然后通过 ARP 获得 MAC 地址，将源 MAC 和目的 MAC 放入 MAC 头中，发出去就可以了。

**如果不是同一网段**，这就需要发往默认网关  Gateway。Gateway 的地址一定是和源 IP 地址是一个网段的。往往不是第一个，就是第二个。例如 192.168.1.0/24  这个网段，Gateway 往往会是 192.168.1.1/24 或者 192.168.1.2/24。

如何发往默认网关呢？网关不是和源 IP 地址是一个网段的么？这个过程就和发往同一个网段的其他机器是一样的：将源地址和目标 IP 地址放入  IP 头中，通过 ARP 获得网关的 MAC 地址，将源 MAC 和网关的 MAC 放入 MAC 头中，发送出去。网关所在的端口，例如  192.168.1.1/24 将网络包收进来，然后接下来怎么做，就完全看网关的了。

**网关往往是一个路由器，是一个三层转发的设备。**啥叫三层设备？前面也说过了，就是把 MAC 头和 IP 头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。

很多情况下，人们把网关就叫作路由器。其实不完全准确，而另一种比喻更加恰当：**路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的 IP 地址都和局域网的 IP 地址相同的网段，每只手都是它握住的那个局域网的网关。**

任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下 MAC 头和 IP 头，看看，根据自己的路由算法，选择另一只手，加上 IP 头和 MAC 头，然后扔出去。



### 静态路由

**静态路由，其实就是在路由器上，配置一条一条规则。**每当要选择从哪抛出去的时候，就一条一条的匹配规则，找到符合的规则，就按规则中设置的那样，从某个口抛出去，找下一跳 IPX。

> #### IP 头和 MAC 头哪些变、哪些不变？

MAC 地址是一个局域网内才有效的地址。因而，MAC 地址只要过网关，就**必定会改变**，因为已经换了局域网。两者主要的区别在于 IP 地址是否改变。不改变 IP 地址的网关，我们称为**转发网关；**改变 IP 地址的网关，我们称为**NAT 网关**。

- **转发网关**

  每到一个新的局域网，MAC 都是要变的，但是 IP 地址都不变。在 IP 头里面，不会保存任何网关的 IP 地址。所谓的下一跳是，某个 IP 要将这个 IP 地址转换为 MAC 放入 MAC 头。在整个过程中，IP 头里面的地址都是不变的。IP 地址在三个局域网都可见，在三个局域网之间的网段都不会冲突。在三个网段之间传输包，IP 头不改变。

- **NAT 网关**

  IP 地址也变，这个过程用英文说就是**Network Address Translation**，简称**NAT**。

  转发网关遇见的第一个问题是，局域网之间没有商量过，各定各的网段，因而 IP 段冲突了。最左面大唐的地址是  192.168.1.101，最右面印度的地址也是 192.168.1.101，如果单从 IP 地址上看，简直是自己访问自己，其实是大唐的  192.168.1.101 要访问印度的 192.168.1.101。

  包在公网与私网传递时需要进行IP转换，转换规则是在路由器里面写定了的。



### 动态路由

> 第9讲 | 路由协议：西出网关无故人，敢问路在何方      



> #### 如何配置路由？

路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为**路由表**。

一张路由表中会有多条路由规则。每一条规则至少包含这三项信息。

- 目的网络：这个包想去哪儿？
- 出口设备：将包从哪个口扔出去？
- 下一跳网关：下一个路由器的地址。

通过 route 命令和 ip route 命令都可以进行查询或者配置。



> #### 动态路由算法

使用动态路由路由器，可以根据路由协议算法生成动态路由表，随网络运行状况的变化而变化。我们都可以将复杂的路径，抽象为一种叫作图的数据结构。路越少越好，道路越短越好，因而这就转化成为**如何在途中找到最短路径**的问题，最短路径常用的有两种方法，一种是 Bellman-Ford 算法，一种是 Dijkstra 算法。



> #### 距离矢量路由算法

第一大类的算法称为**距离矢量路由**（**distance vector routing**）。它是基于 Bellman-Ford 算法的。

这种算法的基本思路是，每个路由器都保存一个路由表，包含多行，每行对应网络中的一个路由器，每一行包含两部分信息，一个是要到目标路由器，从那条线出去，另一个是到目标路由器的距离。

由此可以看出，每个路由器都是知道全局信息的。那这个信息如何更新呢？每个路由器都知道自己和邻居之间的距离，每过几秒，每个路由器都将自己所知的到达所有的路由器的距离告知邻居，每个路由器也能从邻居那里得到相似的信息。

每个路由器根据新收集的信息，计算和其他路由器的距离，比如自己的一个邻居距离目标路由器的距离是 M，而自己距离邻居是 x，则自己距离目标路由器是 x+M。



**存在问题：**

**第一个问题就是好消息传得快，坏消息传得慢。**如果有个路由器加入了这个网络，它的邻居就能很快发现它，然后将消息广播出去。要不了多久，整个网络就都知道了。但是一旦一个路由器挂了，挂的消息是没有广播的。当每个路由器发现原来的道路到不了这个路由器的时候，感觉不到它已经挂了，而是试图通过其他的路径访问，直到试过了所有的路径，才发现这个路由器是真的挂了。

**第二个问题是，每次发送的时候，要发送整个全局路由表。**网络大了，谁也受不了，所以最早的路由协议 RIP 就是这个算法。它适用于小型网络（小于 15 跳）。当网络规模都小的时候，没有问题。现在一个数据中心内部路由器数目就很多，因而不适用了。



#### BGP协议

基于距离矢量路由算法的 BGP。外网的路由协议，也即国家之间的。我们称为**外网路由协议**（**Border Gateway Protocol**，简称**BGP**）。

国家之间，不光远近的问题，还有政策的问题。对于网络包同样，每个数据中心都设置自己的 Policy。例如，哪些外部的 IP 可以让内部知晓，哪些内部的 IP 可以让外部知晓，哪些可以通过，哪些不能通过。这就好比，虽然从我家里到目的地最近，但是不能谁都能从我家走啊！在网络世界，这一个个国家成为自治系统**AS**（Autonomous System）。自治系统分几种类型。

- Stub AS：对外只有一个连接。这类 AS 不会传输其他 AS 的包。例如，个人或者小公司的网络。
- Multihomed AS：可能有多个连接连到其他的 AS，但是大多拒绝帮其他的 AS 传输包。例如一些大公司的网络。
- Transit AS：有多个连接连到其他的 AS，并且可以帮助其他的 AS 传输包。例如主干网。

每个自治系统都有边界路由器，通过它和外面的世界建立联系。

**BGP 又分为两类，eBGP 和 iBGP。**自治系统间，边界路由器之间使用 eBGP 广播路由。内部网络也需要访问其他的自治系统。边界路由器如何将 BGP 学习到的路由导入到内部网络呢？就是通过运行 iBGP，使得内部的路由器能够找到到达外网目的地的最好的边界路由器。

BGP 协议使用的算法是**路径矢量路由协议**（path-vector protocol）。它是距离矢量路由协议的升级版。

前面说了距离矢量路由协议的缺点。其中一个是收敛慢。在 BGP 里面，除了下一跳 hop 之外，还包括了自治系统 AS  的路径，从而可以避免坏消息传的慢的问题，也即上面所描述的，B 知道 C 原来能够到达 A，是因为通过自己，一旦自己都到达不了 A 了，就不用假设 C 还能到达 A 了。

另外，在路径中将一个自治系统看成一个整体，不区分自治系统内部的路由器，这样自治系统的数目是非常有限的。就像大家都能记住出去玩，从中国出发先到韩国然后到日本，只要不计算细到具体哪一站，就算是发送全局信息，也是没有问题的。



> #### 链路状态路由算法

第二大类算法是**链路状态路由**（**link state routing**），基于 Dijkstra 算法。

这种算法的基本思路是：当一个路由器启动的时候，首先是发现邻居，向邻居 say hello，邻居都回复。然后计算和邻居的距离，发送一个  echo，要求马上返回，除以二就是距离。然后将自己和邻居之间的链路状态包广播出去，发送到整个网络的每个路由器。这样每个路由器都能够收到它和邻居之间的关系的信息。因而，每个路由器都能在自己本地构建一个完整的图，然后针对这个图使用 Dijkstra 算法，找到两点之间的最短路径。

不像距离距离矢量路由协议那样，更新时发送整个路由表。链路状态路由协议**只广播更新的或改变的网络拓扑**，这使得更新信息更小，节省了带宽和 CPU 利用率。而且一旦一个路由器挂了，它的邻居都会广播这个消息，可以使得坏消息迅速收敛。



#### OSPF协议

基于链路状态路由算法的 **OSPF**（**Open Shortest Path First**，**开放式最短路径优先**）就是这样一个基于链路状态路由协议，广泛应用在数据中心中的协议。由于主要用在数据中心内部，用于路由决策，因而称为**内部网关协议**（**Interior Gateway Protocol**，简称**IGP**）。内部网关协议的重点就是找到最短的路径。在一个组织内部，路径最短往往最优。当然有时候 OSPF 可以发现多个最短的路径，可以在这多个路径中进行负载均衡，这常常被称为**等价路由**。





# 传输层

> 第10讲讲UDP协议：因性善而简单，难免碰到“城会玩”

传输层主要就是TCP与UDP被问的最多。



> #### TCP和UDP有哪些区别？

- TCP是面向连接的，UDP是面向无连接的。什么叫面向连接，什么叫无连接呢？

  在互通之前，面向连接的协议会先建立连接。例如，TCP会三次握手，而UDP不会。

- 为什么要建立连接呢？你TCP三次握手，我UDP也可以发三个包玩玩，有什么区别吗？

  **所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。**
  
  

例如，**TCP提供可靠交付**。通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达。我们都知道IP包是没有任何可靠性保证的。但是TCP号称能做到那个连接维护的程序做的事情。而**UDP继承了IP包的特性，不保证不丢失，不保证按顺序到达。**

再如，**TCP是面向字节流的**。发送的时候发的是一个流，没头没尾。IP包可不是一个流，而是一个个的IP包。之所以变成了流，这也是TCP自己的状态维护做的事情。而**UDP继承了IP的特性，基于数据报的，一个一个地发，一个一个地收。**

还有**TCP是可以有拥塞控制的**。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。**UDP就不会，应用让我发，我就发，管它洪水滔天。**

因而**TCP其实是一个有状态服务**，通俗地讲就是有脑子的，里面精确地记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。而**UDP则是无状态服务。**通俗地说是没脑子的，天真无邪的，发出去就发出去了。

我们可以这样比喻，如果MAC层定义了本地局域网的传输行为，IP层定义了整个网络端到端的传输行为，这两层基本定义了这样的基因：网络传输是以包为单位的，**二层叫帧，网络层叫包，传输层叫段**。我们笼统地称为包。包单独传输，自行选路，在不同的设备封装解封装，不保证到达。基于这个基因，生下来的孩子UDP完全继承了这些特性，几乎没有自己的思想。



## TCP

> 第11讲讲TCP协议（上）：因性恶而复杂，先恶后善反轻松

TCP是在不可靠的IP层上实现的可靠的数据传输协议，它主要解决传输的**可靠、有序、无丢失和不重复问题**。

1. TCP是**面向链接**的**传输层**协议。
2. 每一条TCP连接只能有2个端点。
3. 全双工
4. 面向字节流



### TCP包头

![img](截图/计算机网络/TCP包头.jpg)

![TCP报文段首部](截图/计算机网络/TCP报文段首部.png)



1. **序号：**TCP是面向字节流的，传输时是按照字节一个个字节来传输的，所以需要带编号。这个字段的意思是——**本报文段所发送的数据的第一个字节的序号。**

2. **确认号**：期望收到对方的**下一个报文段的数据的第一个字节**的序号。

3. **确认位（ACK）**：只有当`ACK = 1 `时，确认号字段才有效。TCP规定，**在连接建立后所有传送的报文段都必须把ACK置为1**。

4. **同步位（SYN）**；`SYN = 1` 时表明这是一个连接请求或者连接接受报文。
   
   - 当`SYN = 1， ACK = 0` 时表明这是一个连接请求报文。
   - 当`SYN = 1， ACK = 1` 时表明这是一个连接接受报文。
   
5. **终止位（FIN）**：用来释放一个连接。`FIN = 1` 表明此报文段的发送方的数据已发送完毕，并要求释放传输连接。

6. **紧急位（URG）**：设置为1时，首部中的紧急指针有效；为0时，紧急指针没有意义。1表示的是此报文段中有紧急数据，将紧急数据排在普通数据的前面；当接受端收到此报文后后必须先处理紧急数据，而后再处理普通数据。 

7. 推位（PSH）：当发送端将PSH置为1时，TCP会立即创建一个报文并发送。接受端收到PSH为1的报文后就立即将接受缓冲区内数据向上交付给应用程序，而不是等待缓冲区满后再交付。 

8. 复位（RST）：

   RST表示复位，用来异常的关闭连接，发送RST包关闭连接时，不必等缓冲区的包都发出去（不像上面的FIN包），直接就丢弃缓存区的包发送RST包。而接收端收到RST包后，也不必发送ACK包来确认。TCP处理程序会在自己认为的异常时刻发送RST包。

   - 例如，A向B发起连接，但B之上并未监听相应的端口，这时B操作系统上的TCP处理程序会发RST包。

   - 又比如，AB正常建立连接了，正在通讯时，A向B发送了FIN包要求关连接，B发送ACK后，网断了，A通过若干原因放弃了这个连接（例如进程重启）。网通了后，B又开始发数据包，A收到后表示压力很大，不知道这野连接哪来的，就发了个RST包强制把连接关了，B收到后会出现connect reset by peer错误。

   > [TCP中的RST标志(Reset)详解](https://blog.csdn.net/a_tu_/article/details/80389878)

9. **校验和**：

   - TCP校验和是一个端到端的校验和，由发送端计算，然后由接收端验证。其目的是为了发现TCP首部和数据在发送端到接收端之间发生的任何改动。如果接收方检测到校验和有差错，则TCP段会被直接丢弃。
   - TCP校验和覆盖TCP首部和TCP数据，而IP首部中的校验和只覆盖IP的首部，不覆盖IP数据报中的任何数据。
   - TCP的校验和是必需的，而UDP的校验和是可选的。
   - TCP和UDP计算校验和时，都要加上一个12字节的伪首部。

   [TCP校验和的原理和实现](https://blog.csdn.net/zhangskd/article/details/11770647)

10. **紧急指针**：[初步认识TCP协议——TCP的紧急状态](https://blog.csdn.net/dhaiuda/article/details/79128584)



### 三次握手

![三次握手](截图/计算机网络/三次握手.png)



1. 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入**SYN_SENT**状态，等待Server确认。（连接请求不携带数据，但要消耗掉一个序号）。
2. 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入**SYN_RCVD**状态。（**这时server为该TCP连接分配TCP缓存和变量**，确认报文也不携带数据，但要消耗掉一个序号）。
3. 第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，序号字段为j + 1 ，确认号字段为 ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入**ESTABLISHED**状态，完成三次握手，随后Client与Server之间可以开始传输数据了。（这时client为该TCP连接分配TCP缓存和变量）





### 四次挥手

![四次挥手](截图/计算机网络/四次挥手.jpg)



1. 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入**FIN_WAIT_1**状态。
2. 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入**CLOSE_WAIT**状态。
3. 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入**LAST_ACK**状态。
4. 第四次挥手：Client收到FIN后，Client进入**TIME_WAIT**状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入**CLOSED**状态，完成四次挥手。



### TCP状态机

<img src="截图/计算机网络/TCP状态机.jpg" alt="img" style="zoom:80%;" />

在这个图中，加黑加粗的部分，是上面说到的主要流程，其中阿拉伯数字的序号，是连接过程中的顺序，而大写中文数字的序号，是连接断开过程中的顺序。加粗的实线是客户端A的状态变迁，加粗的虚线是服务端B的状态变迁。



### TCP 可靠传输

> 第12讲讲TCP协议（下）：西行必定多妖孽，恒心智慧消磨难            
>
> [万字详文：TCP 拥塞控制详解](https://zhuanlan.zhihu.com/p/144273871)

TCP使**用了校验、序号、确认和重传**等机制来达到这个目的。

1. **序号：**TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 

2. **确认：**TCP默认使用累计确认，即TCP只确认数据流中至第一个丢失字节为止的字节。

3. **校验和：** TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 

4. **重传：**超时和冗余ACK都会导致重传。

   - **自动重传请求 ARQ 协议**

     停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求ARQ。

     **优点：** 简单

     **缺点：** 信道利用率低

   - **连续ARQ协议**

     连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。

     **优点：** 信道利用率高，容易实现，即使确认丢失，也不必重传。

     **缺点：** 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5条 消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。
   
   - **Selective Acknowledgment**  （**SACK**）。这种方式需要在TCP头里加一个SACK的东西，可以将缓存的地图发送给发送方。例如可以发送ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是7丢了。



#### TCP流量控制

1. 目的是消除发送方使接收方缓存区溢出，因此可以说流量控制是一个速度匹配服务。
2. TCP使用滑动窗口机制来实现流量控制。



为了记录所有发送的包和接收的包，TCP也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的ID一个个排列，根据处理的情况分成四个部分。

第一部分：发送了并且已经确认的。这部分就是你交代下属的，并且也做完了的，应该划掉的。

第二部分：发送了并且尚未确认的。这部分是你交代下属的，但是还没做完的，需要等待做完的回复之后，才能划掉。

第三部分：没有发送，但是已经等待发送的。这部分是你还没有交代给下属，但是马上就要交代的。

第四部分：没有发送，并且暂时还不会发送的。这部分是你还没有交代给下属，而且暂时还不会交代给下属的。

在TCP里，接收端会给发送端报一个窗口的大小，叫**Advertised window**。这个窗口的大小应该等于上面的第二部分加上第三部分，就是已经交代了没做完的加上马上要交代的。超过这个窗口的，接收端做不过来，就不能发送了。

于是，发送端需要保持下面的数据结构。

![img](截图/计算机网络/TCP流量控制.jpg)

对于接收端来讲，它的缓存里记录的内容要简单一些。

第一部分：接受并且确认过的。也就是我领导交代给我，并且我做完的。

第二部分：还没接收，但是马上就能接收的。也即是我自己的能够接受的最大工作量。

第三部分：还没接收，也没法接收的。也即超过工作量的部分，实在做不完。

![img](截图/计算机网络/TCP流量控制接收端.jpg)

- MaxRcvBuffer：最大缓存的量；
- LastByteRead之后是已经接收了，但是还没被应用层读取的；
- NextByteExpected是第一部分和第二部分的分界线。

第二部分的窗口有多大呢？

NextByteExpected和LastByteRead的差其实是还没被应用层读取的部分占用掉的MaxRcvBuffer的量，我们定义为A。

AdvertisedWindow其实是MaxRcvBuffer减去A。

也就是：AdvertisedWindow=MaxRcvBuffer-((NextByteExpected-1)-LastByteRead)。

那第二部分和第三部分的分界线在哪里呢？NextByteExpected加AdvertisedWindow就是第二部分和第三部分的分界线，其实也就是LastByteRead加上MaxRcvBuffer。

其中第二部分里面，由于受到的包可能不是顺序的，会出现空挡，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。



#### TCP拥塞控制

1. 目的是防止过多的数据注入网络中，这样可以使网络中的路由器或者链路不致过载（**包丢失**和**超时重传**）。

2. 四种算法：慢开始、拥塞避免、快重传、快恢复。

   - **慢开始：** 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。
   
   - **拥塞避免：** 拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送方的cwnd加1.
   
   - **快重传与快恢复：**
     在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。快重传算法要求接收方每收到一个失序的报文段后就立即进行重复确认。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。
     
     ![image-20210310163043636](截图/计算机网络/FRR示意图.png)



例子：

一条TCP连接开始，cwnd设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd加一，于是一次能够发送两个；当这两个的确认到来的时候，每个确认cwnd加一，两个确认cwnd加二，于是一次能够发送四个；当这四个的确认到来的时候，每个确认cwnd加一，四个确认cwnd加四，于是一次能够发送八个。可以看出这是**指数性的增长**。

涨到什么时候是个头呢？有一个值ssthresh为65535个字节，当超过这个值的时候，就要小心一点了，不能倒这么快了，可能快满了，再慢下来。

每收到一个确认后，cwnd增加1/cwnd，我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加1/8，八个确认一共cwnd增加1，于是一次能够发送九个，变成了线性增长。

但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。

拥塞的一种表现形式是丢包，需要超时重传，这个时候，将sshresh设为cwnd/2，将cwnd设为1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。

前面我们讲过**快速重传算法**。当接收端发现丢了一个中间包的时候，发送三次前一个包的ACK，于是发送端就会快速的重传，不必等待超时再重传。TCP认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd减半为cwnd/2，然后sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。

![img](截图/计算机网络/TCP快速恢复.jpg)



就像前面说的一样，正是这种知进退，使得时延很重要的情况下，反而降低了速度。但是如果你仔细想一下，TCP的拥塞控制主要来避免的两个现象都是有问题的。

**第一个问题**是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。

**第二个问题**是TCP的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实TCP只要填满管道就可以了，不应该接着填，直到连缓存也填满。

为了优化这两个问题，后来有了**TCP BBR拥塞算法**。它企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。

![img](截图/计算机网络/TCP拥塞情形.jpg)



### 优化

> [TCP性能优化](https://zhuanlan.zhihu.com/p/170582219)
>
> [TCP 性能优化浅析](https://www.jianshu.com/p/25032abec18a)

- **TCP快速打开**

  **TFO(TCP fast open)**允许服务器和客户端在连接建立握手阶段交换数据，从而使应用节省了一个RTT的时延。它通过握手开始时的 SYN 包中的 TFO cookie（一个 TCP 选项）来验证一个之前连接过的客户端。如果验证成功，它可以在三次握手最终的 ACK 包收到之前就开始发送数据。

  

- **流量控制**

  最初的 TCP 规范分配给接收窗口大小的字段是 16 位，也就是 64KB（2 的 16 次方）。实际上，rwnd 的大小应该由 BDP（带宽延迟积） 而定。`BDP(bit) = bandwidth(b/s) * round-trip time(s)`。比如一个 100Mbps 的宽带，RTT 是100 ms，那么 $BDP =  (100 / 8) * 0.1 = 1.25M$。此时，要想提高网络传输吞吐量，rwnd 应该为 1.25 M。

  为了解决这个问题，TCP 窗口缩放（TCP Window Scaling）出现了，它将窗口大小由 16 位扩展到 32 位。接收窗口大小由65 535字节提高到1G字节





## UDP

### UDP包头

![img](截图/计算机网络/UDP包头.jpg)

UDP协议就相当于一个白板，让开发人员有极大的扩展空间。

> #### UDP校验和是怎么算的？

// todo



> #### UDP的三大特点

第一，**简单**，不需大量的数据结构、处理逻辑、包头字段。前提是它相信网络通路默认就是很容易送达的，不容易被丢弃的。

第二，**无校验**。它不会建立连接，虽然有端口号，但是监听在这个地方，谁都可以传给他数据，他也可以传给任何人数据，甚至可以同时传给多个人数据。

第三，**无拥塞控制**。它不会根据网络的情况进行发包的拥塞控制，无论网络丢包丢成啥样了，它该怎么发还怎么发。

 

> #### UDP的三大使用场景

第一，**需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用**。

DHCP就是基于UDP协议的。一般的获取IP地址都是内网请求，而且一次获取不到IP又没事，过一会儿还有机会。PXE可以在启动的时候自动安装操作系统，操作系统镜像的下载使用的TFTP，这个也是基于UDP协议的。在还没有操作系统的时候，客户端拥有的资源很少，不适合维护一个复杂的状态机，而是因为是内网，一般也没啥问题。

第二，**不需要一对一沟通，建立连接，而是可以广播的应用**。

UDP的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP就是一种广播的形式，就是基于UDP协议的，而广播包的格式前面说过了。

对于多播，我们在讲IP地址的时候，讲过一个D类地址，也即组播地址，使用这个地址，可以将包组播给一批机器。当一台机器上的某个进程想监听某个组播地址的时候，需要发送IGMP包，所在网络的路由器就能收到这个包，知道有个机器上有个进程在监听这个组播地址。当路由器收到这个组播地址的时候，会将包转发给这台机器，这样就实现了跨路由器的组播。

在后面云中网络部分，有一个协议VXLAN，也是需要用到组播，也是基于UDP协议的。

第三，**需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候**。

- **网页或者APP的访问**

  原来访问网页和手机APP都是基于HTTP协议的。HTTP协议是基于TCP的，建立连接都需要多次交互，对于时延比较大的目前主流的移动互联网来讲，建立一次连接需要的时间会比较长，然而既然是移动中，TCP可能还会断了重连，也是很耗时的。而且目前的HTTP协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是TCP的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。

  而**QUIC**（全称**Quick UDP Internet Connections**，**快速UDP互联网连接**）是Google提出的一种基于UDP改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。QUIC在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制。

- **流媒体的协议**

  直播协议多使用RTMP，而这个RTMP协议也是基于TCP的。TCP的严格顺序传输要保证前一个收到了，下一个才能确认，如果前一个收不到，下一个就算包已经收到了，在缓存里面，也需要等着。对于直播来讲，这显然是不合适的，因为老的视频帧丢了其实也就丢了，就算再传过来用户也不在意了，他们要看新的了，如果老是没来就等着，卡顿了，新的也看不了，那就会丢失客户，所以直播，实时性比较比较重要，宁可丢包，也不要卡顿的。

  另外，对于丢包，其实对于视频播放来讲，有的包可以丢，有的包不能丢，因为视频的连续帧里面，有的帧重要，有的不重要，如果必须要丢包，隔几个帧丢一个，其实看视频的人不会感知，但是如果连续丢帧，就会感知了，因而在网络不好的情况下，应用希望选择性的丢帧。

  还有就是当网络不好的时候，TCP协议会主动降低发送速度，这对本来当时就卡的看视频来讲是要命的，应该应用层马上重传，而不是主动让步。因而，很多直播应用，都基于UDP实现了自己的视频传输协议。

- **实时游戏**

  游戏有一个特点，就是实时性比较高。因而，实时游戏中客户端和服务端要建立长连接，来保证实时传输。但是游戏玩家很多，服务器却不多。由于维护TCP连接需要在内核维护一些数据结构，因而一台机器能够支撑的TCP连接数目是有限的，然后UDP由于是没有连接的，在异步IO机制引入之前，常常是应对海量客户端连接的策略。

  另外还是TCP的强顺序问题，对战的游戏，对网络的要求很简单，玩家通过客户端发送给服务器鼠标和键盘行走的位置，服务器会处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。

  如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，然而玩家并不关心过期的数据，激战中卡1秒，等能动了都已经死了。

  游戏对实时要求较为严格的情况下，采用自定义的可靠UDP协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。

- **“城会玩”四：IoT物联网**

  一方面，物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护TCP协议代价太大；另一方面，物联网对实时性要求也很高，而TCP还是因为上面的那些原因导致时延大。Google旗下的Nest建立Thread Group，推出了物联网通信协议Thread，就是基于UDP协议的。

- **“城会玩”五：移动通信领域**

  在4G网络里，移动流量上网的数据面对的协议GTP-U是基于UDP的。因为移动网络协议比较复杂，而GTP协议本身就包含复杂的手机上线下线的通信协议。如果基于TCP，TCP的机制就显得非常多余，这部分协议我会在后面的章节单独讲解。





> #### 问题

> ##### 使用UDP协议来保证一个可靠的传输

// todo



# 应用层

## DHCP

> 第04讲丨DHCP与PXE：IP是怎么来的，又是怎么没的？

**动态主机配置协议（Dynamic Host Configuration Protocol）**，简称**DHCP**

> #### 解析 DHCP 的工作方式

**DHCP Discover。**

新来的机器使用 IP 地址 0.0.0.0 发送了一个广播包，目的 IP 地址为 255.255.255.255。广播包封装了UDP，UDP封装了BOOTP。其实 DHCP 是 BOOTP 的增强版，但是如果你去抓包的话，很可能看到的名称还是 BOOTP 协议。在这个广播包里面，新人大声喊：我是新来的（Boot request），我的 MAC 地址是这个，我还没有 IP，谁能给租给我个 IP 地址！

Boot request 结构

![index](截图/计算机网络/DHCP包结构.jpg)



**DHCP Offer**

如果一个网络管理员在网络里面配置了**DHCP Server**的话，立刻能知道来了一个“新人”。由于MAC 唯一，IP 管理员知道这是一个新人，需要租给它一个 IP 地址，这个过程我们称为**DHCP Offer**。同时，DHCP Server 为此客户保留为它提供的 IP 地址，从而不会为其他 DHCP 客户分配此 IP 地址。

DHCP Offer 的格式就像这样，里面有给新人分配的地址。

![index](截图/计算机网络/DHCP Offer格式.jpg)

DHCP Server 仍然使用广播地址作为目的地址，因为，此时请求分配 IP 的新人还没有自己的 IP。除此之外，服务器还发送了子网掩码、网关和 IP 地址租用期等信息。

如果有多个 DHCP Server，这台新机器会收到多个 IP 地址。它会选择其中一个 DHCP Offer，一般是最先到达的那个，并且会向网络发送一个 DHCP Request 广播数据包，包中包含客户端的 MAC 地址、接受的租约中的 IP 地址、提供此租约的 DHCP 服务器地址等，并告诉所有 DHCP Server 它将接受哪一台服务器提供的 IP 地址，告诉其他 DHCP 服务器，谢谢你们的接纳，并请求撤销它们提供的 IP 地址，以便提供给下一个 IP 租用请求者。

此时，由于还没有得到 DHCP Server 的最后确认，客户端仍然使用 0.0.0.0 为源 IP 地址、255.255.255.255 为目标地址进行广播。在 BOOTP 里面，接受某个 DHCP Server 的分配的 IP。

当 DHCP Server 接收到客户机的 DHCP request 之后，会广播返回给客户机一个 DHCP ACK 消息包，表明已经接受客户机的选择，并将这一 IP 地址的合法租用信息和其他的配置信息都放入该广播包，发给客户机。最终租约达成的时候，还是需要广播一下，让大家都知道。



> #### IP 地址的收回和续租

客户机会在租期过去 50% 的时候，直接向为其提供 IP 地址的 DHCP Server 发送 DHCP request  消息包。客户机接收到该服务器回应的 DHCP ACK 消息包，会根据包中所提供的新的租期以及其他已经更新的 TCP/IP  参数，更新自己的配置。这样，IP 租用更新就完成了。



## HTTP 

> 第14讲 | HTTP协议：看个新闻原来这么麻烦

### HTTP 1.1

目前使用的 HTTP 协议大部分都是 1.1。在 1.1 的协议里面，默认是开启了 Keep-Alive 的，这样建立的 TCP 连接，就可以在多次请求中复用。

> #### 请求报文

![img](截图/计算机网络/Http报文结构.jpeg)

HTTP 的报文大概分为三大部分。第一部分是**请求行**，第二部分是请求的**首部**，第三部分才是请求的**正文实体**。

> #### 首部字段

请求行下面就是首部字段。首部是 key value，通过冒号分隔。这里面，往往保存了一些非常重要的字段。

例如，**Accept-Charset**，表示**客户端可以接受的字符集**。防止传过来的是另外的字符集，从而导致出现乱码。再如，**Content-Type**是指**正文的格式**。例如，我们进行 POST 的请求，如果正文是 JSON，那么我们就应该将这个值设置为 JSON。





> #### 返回报文

![img](截图/计算机网络/Http返回报文格式.jpeg)

状态码会反应 HTTP 请求的结果。“200”、“404”等等。然后，短语会大概说一下原因。接下来是返回首部的**key value**。返回的头部里面也会有**Content-Type**，表示返回的是 HTML，还是 JSON。



> ####  Pipelining

HTTP Pipelining是这样一种技术：在等待上一个请求响应的同时，发送下一个请求。(译者注：作者这个解释并不完全正确，HTTP  Pipelining其实是把多个HTTP请求放到一个TCP连接中一一发送，而在发送过程中不需要等待服务器对前一个请求的响应；只不过，客户端还是要按照发送请求的顺序来接收响应。)但就像在超市收银台或者银行柜台排队时一样，你并不知道前面的顾客是干脆利索的还是会跟收银员/柜员磨蹭到世界末日（译者注：不管怎么说，服务器（即收银员/柜员）是要按照顺序处理请求的，如果前一个请求非常耗时（顾客磨蹭），那么后续请求都会受到影响），这就是所谓的线头阻塞（Head of line blocking）。 

当然，你可以去排一个你认为最快的队伍，或者甚至另起一个新的队伍（译者注：即新建一个TCP连接）。但不管怎么样，你总归得先选择一个队伍，而且一旦选定之后，就不能更换队伍。

但是，另起新队伍会导致资源耗费和性能损失（译者注：新建 TCP  连接的开销非常大）。这种另起新队伍的方式只在新队伍数量很少的情况下有作用，因此它并不具备可扩展性。（译者注：这段话意思是说，靠大量新建连接是不能有效解决延迟问题的，即HTTP Pipelining并不能彻底解决Head ofline blocking问题。）所以针对此问题并没有完美的解决方案。

这就是为什么，即使在2015年的今天，大部分桌面浏览器仍然会选择默认关闭HTTP pipelining这一功能的原因。



> #### 存在的5大问题

- 队头阻塞：在大多数情况下，浏览器获取资源都是多份的，HTTP/1.1并没有提供机制来同时请求所有的资源，在仅仅只有一个连接的时候(实际情况，现代浏览器可能针对单个域名建立多个连接，实现某程度并行)，它需要发起请求，并等待响应。在这之后再发起下一请求，请求响应。即使HTTP/1.1有一个特性为管道化，允许发送一组请求，但是只能按照发送顺序依次接收响应；如果在请求应答过程中，出现任何的状况，剩下的所有请求都会被堵塞在出现问题的请求应答之后，这就是"队头阻塞";

- 低效的TCP利用：TCP协议作为最可靠的协议之一，其核心就是拥塞窗口[拥塞窗口指的是在接收方确认数据包之前,发送方可以发出]的TCP包的数量]；拥塞控制能防止过多的数据注入到网络中，避免网络过载。TCP中可以通过慢启动探索当前连接对应拥塞窗口的合适大小，即发送者发送数据的时候并非一开始注入大量数据到网络中,，而是发送一个数据包进行测试，当得到确认回复后,额外发送一个未确认包，即得到一个确认回复，可以发送两个数据包，得到两个确认回复可以发送四个数据包，以几何形式增长便可很快到达协议规定的拥塞窗口规定，这时候连接进入拥塞避免阶段；这种机制需要往返几次才能得知最佳拥塞窗口大小，但往返几次所需的时间成本不可忽略;

- 臃肿的消息首部:HTTP/1.1能压缩请求内容，但是消息首部不能压缩；在现今请求中，消息首部占请求绝大部分(甚至是全部)也较为常见。

- 受限的优先级设置:即如果浏览器针对指定域名开启多个socket请求，若web页面某些资源比另外一些资源重要，这会加重资源的排队效应；即先请求优先级高的资源，在获取之后，再请求优先级较低的资源，并且在请求优先级高的资源的时间区间内浏览器并不会发起优先级较低的新请求.

- 第三方资源:资源完全独立于站点服务器的控制,称为第三方资源. 

  

> [HTTP/1.1的性能问题及可优化方案](https://blog.csdn.net/u012205779/article/details/79319268)



### HTTP 2.0

**HTTP 2.0 通过头压缩、分帧、二进制编码、多路复用等技术提升性能；**

HTTP 1.1 在应用层以纯文本的形式进行通信。每次通信都要带完整的 HTTP 的头，而且不考虑 pipeline 模式的话，每次的过程总是像上面描述的那样一去一回。这样在实时性、并发性上都存在问题。

为了解决这些问题，HTTP 2.0 会对 HTTP 的头进行一定的压缩，将原来每次都要携带的大量 key  value 在两端建立一个索引表，对相同的头只发送索引表中的索引。

另外，HTTP 2.0 协议将一个 TCP 的连接中，切分成多个流，每个流都有自己的 ID，而且流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。流是有优先级的。

HTTP 2.0 还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码，所有数据以二进制进行传输。常见的帧有**Header 帧**，用于传输 Header 内容，并且会开启一个新的流。再就是**Data 帧**，用来传输正文实体。多个 Data 帧属于同一个流。

通过这两种机制，HTTP 2.0 的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。

HTTP 2.0 成功解决了 HTTP 1.1 的队首阻塞问题，同时，也不需要通过 HTTP 1.x 的 pipeline 机制用多条 TCP  连接来实现并行请求与响应；减少了 TCP 连接数对服务器性能的影响，同时将页面的多个数据 css、js、 jpg  等通过一个数据链接进行传输，能够加快页面组件的传输速度。



> #### 多路复用

HTTP/1.1中的消息是“管道串形化”的：只有等一个消息完成之后，才能进行下一条消息；而HTTP/2中多个消息交织在了一起，这无疑提高了“通信”的效率。这就是多路复用：**在一个HTTP的连接上，多路“HTTP消息”同时工作**。

**HTTP/2是基于二进制“帧”的协议，HTTP/1.1是基于“文本分割”解析的协议**，所以HTTP/1.1不能实现“多路复用”，因为：

- 一次只能处理一个请求或响应，因为这种以分隔符分割消息的数据，在完成之前不能停止解析。
- 解析这种数据无法预知需要多少内存，这会带给“服务端”很大的压力，因为它不知道要把一行要解析的内容读到多大的“缓冲区”中，在保证解析效率和速度的前提下：内存该如何分配



[HTTP/2协议“多路复用”实现原理](https://blog.csdn.net/weixin_33693070/article/details/88711831)



> #### 缺陷

HTTP 2.0 虽然大大增加了并发性，但还是有问题的。因为 HTTP 2.0 也是基于 TCP 协议的，TCP 协议在处理包时是有严格顺序的。

当其中一个数据包遇到问题，TCP 连接需要等待这个包完成重传之后才能继续进行。虽然 HTTP 2.0 通过多个  stream，使得逻辑上一个 TCP 连接上的并行内容，进行多路数据的传输，然而这中间并没有关联的数据。一前一后，前面 stream 2  的帧没有收到，后面 stream 1 的帧也会因此阻塞。



> #### HTTP1.0和HTTP1.1和HTTP2.0的区别

**HTTP1.0和HTTP1.1的区别：**

- 长连接(Persistent Connection)

  HTTP1.1支持长连接和请求的流水线处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启长连接keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。HTTP1.0需要使用keep-alive参数来告知服务器端要建立一个长连接。

- 节约带宽

  HTTP1.0中存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能。HTTP1.1支持只发送header信息（不带任何body信息），如果服务器认为客户端有权限请求服务器，则返回100，客户端接收到100才开始把请求body发送到服务器；如果返回401，客户端就可以不用发送请求body了节约了带宽。

- HOST域

  在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname），HTTP1.0没有host域。随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都支持host域，且请求消息中如果没有host域会报告一个错误（400 Bad Request）。

- 缓存处理

  在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。

- 错误通知的管理

  在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。



**HTTP1.1和HTTP2.0的区别**

- 多路复用

  HTTP2.0使用了多路复用的技术，做到同一个连接并发处理多个请求。HTTP1.1也可以多建立几个TCP连接，来支持处理更多并发的请求，但是创建TCP连接本身也是有开销的。即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。多路复用

- 头部数据压缩

  在HTTP1.1中，HTTP请求和响应都是由状态行、请求/响应头部、消息主体三部分组成。一般而言，消息主体都会经过gzip压缩，或者本身传输的就是压缩过后的二进制文件，但状态行和头部却没有经过任何压缩，直接以纯文本传输。随着Web功能越来越复杂，每个页面产生的请求数也越来越多，导致消耗在头部的流量越来越多，尤其是每次都要传输UserAgent、Cookie这类不会频繁变动的内容，完全是一种浪费。HTTP1.1不支持header数据的压缩，HTTP2.0使用HPACK算法对header的数据进行压缩，这样数据体积小了，在网络上传输就会更快。

- 服务器推送

  服务端推送是一种在客户端请求之前发送数据的机制。网页使用了许多资源：HTML、样式表、脚本、图片等等。在HTTP1.1中这些资源每一个都必须明确地请求。这是一个很慢的过程。浏览器从获取HTML开始，然后在它解析和评估页面的时候，增量地获取更多的资源。因为服务器必须等待浏览器做每一个请求，网络经常是空闲的和未充分使用的。 为了改善延迟，HTTP2.0引入了server push，它允许服务端推送资源给浏览器，在浏览器明确地请求之前，免得客户端再次创建连接发送请求到服务器端获取。这样客户端可以直接从本地加载这些资源，不用再通过网络。

- 新的二进制格式（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。



> [HTTP1.0和HTTP1.1和HTTP2.0的区别](https://blog.csdn.net/ailunlee/article/details/97831912)
>
> [HTTPS和HTTP2.0详解](https://juejin.cn/post/6844903608635359239)
>
> [回顾 HTTP1.0，HTTP1.1，HTTP2.0的区别](https://juejin.cn/post/6904423377499324423)





### HTTP 3（QUIC）

> #### 机制一：自定义连接机制

我们都知道，一条 TCP 连接是由四元组标识的，分别是源 IP、源端口、目的  IP、目的端口。一旦一个元素发生变化时，就需要断开重连，重新连接。在移动互联情况下，当手机信号不稳定或者在 WIFI 和  移动网络切换时，都会导致重连，从而进行再次的三次握手，导致一定的时延。

这在 TCP 是没有办法的，但是基于 UDP，就可以在 QUIC 自己的逻辑里面维护连接的机制，不再以四元组标识，而是以一个 64  位的随机数作为 ID 来标识，而且 UDP 是无连接的，所以当 IP 或者端口变化的时候，只要 ID 不变，就不需要重新建立连接。



> #### 机制二：自定义重传机制

TCP 为了保证可靠性，通过使用**序号**和**应答**机制，来解决顺序问题和丢包问题。

任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发这个序号的包。那怎么样才算超时呢？还记得我们提过的**自适应重传算法**吗？这个超时是通过**采样往返时间 RTT**不断调整的。

其实，在 TCP 里面超时的采样存在不准确的问题。例如，发送一个包，序号为 100，发现没有返回，于是再发送一个 100，过一阵返回一个  ACK101。这个时候客户端知道这个包肯定收到了，但是往返时间是多少呢？是 ACK 到达的时间减去后一个 100 发送的时间，还是减去前一个  100 发送的时间呢？事实是，第一种算法把时间算短了，第二种算法把时间算长了。

QUIC 也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是  100，发现没有返回；再次发送的时候，序号就是 101 了；如果返回的 ACK  100，就是对第一个包的响应。如果返回 ACK  101  就是对第二个包的响应，RTT 计算相对准确。

但是这里有一个问题，就是怎么知道包 100 和包 101 发送的是同样的内容呢？QUIC 定义了一个 offset 概念。QUIC  既然是面向连接的，也就像 TCP 一样，是一个数据流，发送的数据在这个数据流里面有个偏移量 offset，可以通过 offset  查看数据发送到了哪里，这样只要这个 offset 的包没有来，就要重发；如果来了，按照 offset 拼接，还是能够拼成一个流。

![index](截图/计算机网络/QUIC重传机制.jpg)

### 机制三：无阻塞的多路复用

有了自定义的连接和重传机制，我们就可以解决上面 HTTP  2.0 的多路复用问题。

同 HTTP 2.0 一样，同一条 QUIC 连接上可以创建多个 stream，来发送多个 HTTP 请求。但是，QUIC 是基于 UDP 的，一个连接上的多个 stream 之间没有依赖。这样，假如 stream2 丢了一个 UDP 包，后面跟着 stream3 的一个 UDP  包，虽然 stream2 的那个包需要重传，但是 stream3 的包无需等待，就可以发给用户。

### 机制四：自定义流量控制

TCP 的流量控制是通过**滑动窗口协议**。QUIC 的流量控制也是通过 window_update，来告诉对端它可以接受的字节数。但是 QUIC 的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个 stream 控制窗口。

还记得吗？在 TCP 协议中，接收端的窗口的起始点是下一个要接收并且 ACK 的包，即便后来的包都到了，放在缓存里面，窗口也不能右移，因为 TCP 的 ACK 机制是基于序列号的累计应答，一旦 ACK 了一个系列号，就说明前面的都到了，所以只要前面的没到，后面的到了也不能  ACK，就会导致后面的到了，也有可能超时重传，浪费带宽。

QUIC 的 ACK 是基于 offset 的，每个 offset  的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空挡会等待到来或者重发即可，而窗口的起始位置为当前收到的最大 offset，从这个  offset 到当前的 stream 所能容纳的最大缓存，是真正的窗口大小。显然，这样更加准确。

![img](截图/计算机网络/QUIC发送端窗口.jpeg)

另外，还有整个连接的窗口，需要对于所有的 stream 的窗口做一个统计。



腾讯公众号有一篇非常好的

[HTTP/3原理与实践](https://mp.weixin.qq.com/s/iF0wbV5o7HVjGG_Cb-RcOg)



## HTTPS



> #### 数字证书

如何将不对称加密的公钥给对方呢？一种是放在一个公网的地址上，让对方下载；另一种就是在建立连接的时候，传给对方。这两种方法有相同的问题，那就是，作为一个普通网民，你怎么鉴别别人给你的公钥是对的。

这个时候就需要权威部门的介入了，这个由权威部门颁发的称为**证书**（**Certificate**）。证书里面有**公钥**，这是最重要的；还有证书的**所有者**；另外还有证书的**发布机构**和证书的**有效期**。

这个证书是怎么生成的呢？会不会有人假冒权威机构颁发证书呢？生成证书需要发起一个证书请求，然后将这个请求发给一个权威机构去认证，这个权威机构我们称为**CA**（ **Certificate Authority**）。

证书请求可以通过这个命令生成。

```sh
openssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.req
```

将这个请求发给权威机构，权威机构会给这个证书卡一个章，我们称为 **签名算法。** 问题又来了，那怎么签名才能保证是真的权威机构签名的呢？当然只有用只掌握在权威机构手里的东西签名了才行，这就是 CA 的私钥。

签名算法大概是这样工作的：一般是对信息做一个 Hash 计算，得到一个 Hash 值，这个过程是不可逆的，也就是说无法通过 Hash 值得出原来的信息内容。在把信息发送出去时，把这个 Hash 值加密后，作为一个签名和信息一起发出去。CA 用自己的私钥给网站的公钥签名，就相当于给网站背书，形成了网站的证书。

证书内容有 Issuer，也即证书是谁颁发的；Subject，就是证书颁发给谁；Validity 是证书期限；Public-key 是公钥内容；Signature Algorithm 是签名算法。

这下好了，你不会从外卖网站上得到一个公钥，而是会得到一个证书，这个证书有个发布机构 CA，你只要得到这个发布机构 CA 的公钥，去解密外卖网站证书的签名，如果解密成功了，Hash 也对的上，就说明这个外卖网站的公钥没有啥问题。

你有没有发现，又有新问题了。要想验证证书，需要 CA 的公钥，问题是，你怎么确定 CA 的公钥就是对的呢？

所以，CA 的公钥也需要更牛的 CA 给它签名，然后形成 CA 的证书。要想知道某个 CA 的证书是否可靠，要看 CA  的上级证书的公钥，能不能解开这个 CA  的签名。就像你不相信区公安局，可以打电话问市公安局，让市公安局确认区公安局的合法性。这样层层上去，直到全球皆知的几个著名大 CA，称为**root CA**，做最后的背书。通过这种**层层授信背书**的方式，从而保证了非对称加密模式的正常运转。

除此之外，还有一种证书，称为**Self-Signed Certificate**，就是自己给自己签名。



> #### HTTPS 的工作模式

![index](截图/计算机网络/HTTPS工作流程.jpg)

> #### 重放与篡改

有了加密和解密，黑客截获了包也打不开了，但是它可以发送 N 次。这个往往通过 Timestamp 和 Nonce 随机数联合起来，然后做一个不可逆的签名来保证。

Nonce 随机数保证唯一，或者 Timestamp 和 Nonce 合起来保证唯一，同样的，请求只接受一次，于是服务器多次受到相同的 Timestamp 和 Nonce，则视为无效即可。

如果有人想篡改 Timestamp 和 Nonce，还有签名保证不可篡改性，如果改了用签名算法解出来，就对不上了，可以丢弃了。



# DNS

## 传统DNS

> 第18讲 | DNS协议：网络世界的地址簿      

> #### 树状的层次结构

![img](截图/计算机网络/DNS树状结构.jpeg)

- 根 DNS 服务器 ：返回顶级域 DNS 服务器的 IP 地址
- 顶级域 DNS 服务器：返回权威 DNS 服务器的 IP 地址
- 权威 DNS 服务器 ：返回相应主机的 IP 地址



> #### DNS 解析流程

为了提高 DNS 的解析性能，很多网络都会就近部署 DNS 缓存服务器。于是，就有了以下的 DNS 解析流程。

1. 电脑客户端会发出一个 DNS 请求，问 www.163.com 的 IP 是啥啊，并发给本地域名服务器 (本地  DNS)。那本地域名服务器 (本地 DNS) 是什么呢？如果是通过 DHCP 配置，本地 DNS  由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。
2. 本地 DNS 收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应 IP 地址的大表格。如果能找到  www.163.com，它直接就返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器，根域名服务器是最高层次的，全球共有 13 套。它不直接用于域名解析，但能指明一条道路。
3. 根 DNS 收到来自本地 DNS 的请求，发现后缀是 .com，说：“哦，www.163.com 啊，这个域名是由.com 区域管理，我给你它的顶级域名服务器的地址，你去问问它吧。”
4. 本地 DNS 转向问顶级域名服务器：“老二，你能告诉我 www.163.com 的 IP 地址吗？”顶级域名服务器就是大名鼎鼎的比如 .com、.net、 .org 这些一级域名，它负责管理二级域名，比如 163.com，所以它能提供一条更清晰的方向。
5. 顶级域名服务器说：“我给你负责 www.163.com 区域的权威 DNS 服务器的地址，你去问它应该能问到。”
6. 本地 DNS 转向问权威 DNS 服务器：“您好，www.163.com 对应的 IP 是啥呀？”163.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。
7. 权限 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。
8. 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。

![index](截图/计算机网络/DNS解析流程.jpg)

> #### 负载均衡

通过 DNS 访问数据中心中对象存储上的静态资源为例，看一看整个过程。

假设全国有多个数据中心，托管在多个运营商，每个数据中心三个可用区（Available  Zone）。对象存储通过跨可用区部署，实现高可用性。在每个数据中心中，都至少部署两个内部负载均衡器，内部负载均衡器后面对接多个对象存储的前置服务器（Proxy-server）。

![index](截图/计算机网络/DNS负载均衡.jpg)

1. 当一个客户端要访问 object.yourcompany.com 的时候，需要将域名转换为 IP 地址进行访问，所以它要请求本地 DNS 解析器。
2. 本地 DNS 解析器先查看看本地的缓存是否有这个记录。如果有则直接使用，因为上面的过程太复杂了，如果每次都要递归解析，就太麻烦了。
3. 如果本地无缓存，则需要请求本地的 DNS 服务器。
4. 本地的 DNS 服务器一般部署在你的数据中心或者你所在的运营商的网络中，本地 DNS 服务器也需要看本地是否有缓存，如果有则返回，因为它也不想把上面的递归过程再走一遍。
5. 至 7. 如果本地没有，本地 DNS 才需要递归地从根 DNS 服务器，查到.com 的顶级域名服务器，最终查到 yourcompany.com 的权威 DNS 服务器，给本地 DNS 服务器，权威 DNS 服务器按说会返回真实要访问的 IP 地址。

对于不需要做全局负载均衡的简单应用来讲，yourcompany.com 的权威 DNS 服务器可以直接将  object.yourcompany.com 这个域名解析为一个或者多个 IP 地址，然后客户端可以通过多个 IP  地址，进行简单的轮询，实现简单的负载均衡。

但是对于复杂的应用，尤其是跨地域跨运营商的大型应用，则需要更加复杂的全局负载均衡机制，因而需要专门的设备或者服务器来做这件事情，这就是**全局负载均衡器**（**GSLB**，**Global Server Load Balance**）。

在 yourcompany.com 的 DNS 服务器中，一般是通过**配置 CNAME 的方式**，给  object.yourcompany.com 起一个别名，例如 object.vip.yourcomany.com，然后告诉本地 DNS  服务器，让它请求 GSLB 解析这个域名，GSLB 就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。

图中画了两层的 GSLB，是因为分运营商和地域。我们希望不同运营商的客户，可以访问相同运营商机房中的资源，这样不跨运营商访问，有利于提高吞吐量，减少时延。

1. 第一层 GSLB，通过查看请求它的本地 DNS 服务器所在的运营商，就知道用户所在的运营商。假设是移动，通过 CNAME 的方式，通过另一个别名 object.yd.yourcompany.com，告诉本地 DNS 服务器去请求第二层的 GSLB。
2. 第二层 GSLB，通过查看请求它的本地 DNS 服务器所在的地址，就知道用户所在的地理位置，然后将距离用户位置比较近的 Region 里面，六个**内部负载均衡**（**SLB**，S**erver Load Balancer**）的地址，返回给本地 DNS 服务器。
3. 本地 DNS 服务器将结果返回给本地 DNS 解析器。
4. 本地 DNS 解析器将结果缓存后，返回给客户端。
5. 客户端开始访问属于相同运营商的距离较近的 Region 1 中的对象存储，当然客户端得到了六个 IP 地址，它可以通过负载均衡的方式，随机或者轮询选择一个可用区进行访问。对象存储一般会有三个备份，从而可以实现对存储读写的负载均衡。



> #### 存在问题

1. **域名缓存问题**

   它可以在本地做一个缓存，也就是说，不是每一个请求，它都会去访问权威 DNS 服务器，而是访问过一次就把结果缓存到自己本地，当其他人来问的时候，直接就返回这个缓存数据。刷新缓存的时延问题

   再就是本地的缓存，往往使得全局负载均衡失败，因为上次进行缓存的时候，缓存中的地址不一定是这次访问离客户最近的地方，如果把这个地址返回给客户，那肯定就会绕远路。

2. **域名转发问题**

   缓存问题还是说本地域名解析服务，还是会去权威 DNS 服务器中查找，只不过不是每次都要查找。可以说这还是大导游、大中介。还有一些小导游、小中介，有了请求之后，直接转发给其他运营商去做解析，自己只是外包了出去。

   这样的问题是，如果是 A 运营商的客户，访问自己运营商的 DNS 服务器，如果 A 运营商去权威 DNS 服务器查询的话，权威 DNS 服务器知道你是 A 运营商的，就返回给一个部署在 A 运营商的网站地址，这样针对相同运营商的访问，速度就会快很多。

   但是 A 运营商偷懒，将解析的请求转发给 B 运营商，B 运营商去权威 DNS 服务器查询的话，权威服务器会误认为，你是 B 运营商的，那就返回给你一个在 B 运营商的网站地址吧，结果客户的每次访问都要跨运营商，速度就会很慢。

3. **出口 NAT 问题**

   前面讲述网关的时候，我们知道，出口的时候，很多机房都会配置**NAT**，也即**网络地址转换**，使得从这个网关出去的包，都换成新的 IP 地址，当然请求返回的时候，在这个网关，再将 IP 地址转换回去，所以对于访问来说是没有任何问题。

   但是一旦做了网络地址的转换，权威的 DNS 服务器，就没办法通过这个地址，来判断客户到底是来自哪个运营商，而且极有可能因为转换过后的地址，误判运营商，导致跨运营商的访问。

4. **域名更新问题**

   本地 DNS 服务器是由不同地区、不同运营商独立部署的。对域名解析缓存的处理上，实现策略也有区别，有的会偷懒，忽略域名解析结果的 TTL  时间限制，在权威 DNS 服务器解析变更的时候，解析结果在全网生效的周期非常漫长。但是有的时候，在 DNS 的切换中，场景对生效时间要求比较高。

5. **解析延迟问题**

   从上一节的 DNS 查询过程来看，DNS 的查询过程需要递归遍历多个 DNS 服务器，才能获得最终的解析结果，这会带来一定的时延，甚至会解析超时。



## HTTPDNS

> 第19讲 | HTTPDNS：网络世界的地址簿也会指错路

**HTTPNDS 其实就是，不走传统的 DNS 解析，而是自己搭建基于 HTTP 协议的 DNS 服务器集群，分布在多个地点和多个运营商。当客户端需要 DNS 解析的时候，直接通过 HTTP 协议进行请求这个服务器集群，得到就近的地址。**



> #### 工作模式

在客户端的 SDK 里动态请求服务端，获取 HTTPDNS 服务器的 IP 列表，缓存到本地。随着不断地解析域名，SDK 也会在本地缓存 DNS 域名解析的结果。

当手机应用要访问一个地址的时候，首先看是否有本地的缓存，如果有就直接返回。这个缓存和本地 DNS 的缓存不一样的是，这个是手机应用自己做的，而非整个运营商统一做的。如何更新、何时更新，手机应用的客户端可以和服务器协调来做这件事情。

如果本地没有，就需要请求 HTTPDNS 的服务器，在本地 HTTPDNS 服务器的 IP 列表中，选择一个发出 HTTP 的请求，会返回一个要访问的网站的 IP 列表。



> #### 缓存设计

> #### 调度设计



# CDN

> 第20讲 | CDN：你去小卖部取过快递么？      

> #### 分发系统的架构

分布在各个地方的各个数据中心的节点，就称为**边缘节点**。

由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。

![index](截图/计算机网络/CDN分发系统架构.jpg)



> #### 访问流程

![index](截图/计算机网络/CDN访问流程.jpg)

**在没有 CDN 的情况下**，用户向浏览器输入 www.web.com 这个域名，客户端访问本地 DNS  服务器的时候，如果本地 DNS 服务器有缓存，则返回网站的地址；如果没有，递归查询到网站的权威 DNS 服务器，这个权威 DNS 服务器是负责  web.com 的，它会返回网站的 IP 地址。本地 DNS 服务器缓存下 IP 地址，将 IP 地址返回，然后客户端直接访问这个 IP  地址，就访问到了这个网站。

然而**有了 CDN 之后，情况发生了变化**。在 web.com 这个权威 DNS 服务器上，会设置一个 CNAME 别名，指向另外一个域名 [www.web.cdn.com](http://www.web.cdn.com)，返回给本地 DNS 服务器。

当本地 DNS 服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是 web.com 的权威 DNS  服务器了，而是 web.cdn.com 的权威 DNS 服务器，这是 CDN 自己的权威 DNS 服务器。在这个服务器上，还是会设置一个  CNAME，指向另外一个域名，也即 CDN 网络的全局负载均衡器。

接下来，本地 DNS 服务器去请求 CDN 的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：

- 根据用户 IP 地址，判断哪一台服务器距用户最近；
- 用户所处的运营商；
- 根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需的内容；
- 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。

基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的 IP 地址。

本地 DNS 服务器缓存这个 IP 地址，然后将 IP  返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。



> #### CDN的流媒体应用

CDN 支持**流媒体协议**，例如 RTMP 协议。在很多情况下，这相当于一个代理，从上一级缓存读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到用户的客户端。

对于静态页面来讲，内容的分发往往采取**拉取**的方式，也即当发现未命中的时候，再去上一级进行拉取。但是，流媒体数据量大，如果出现**回源**，压力会比较大，所以往往采取主动**推送**的模式，将热点数据主动推送到边缘节点。

对于流媒体来讲，很多 CDN 还提供**预处理服务**，也即文件在分发之前，经过一定的处理。例如将视频转换为不同的码流，以适应不同的网络带宽的用户需求；再如对视频进行分片，降低存储压力，也使得客户端可以选择使用不同的码率加载不同的分片。这就是我们常见的，“我要看超清、标清、流畅等”。



对于流媒体 CDN 来讲，有个关键的问题是**防盗链**问题。

最常用也最简单的方法就是**HTTP 头的 refer 字段**， 当浏览器发送请求的时候，一般会带上 referer，告诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果 refer 信息不是来自本站，就阻止访问或者跳到其它链接。

**refer 的机制相对比较容易破解，所以还需要配合其他的机制。**

一种常用的机制是**时间戳防盗链**。使用 CDN 的管理员可以在配置界面上，和 CDN 厂商约定一个加密字符串。

客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然后生成一个下载链接，带上这个签名字符串和截止时间戳去访问 CDN。

在 CDN 服务端，根据取出过期时间，和当前 CDN 节点时间进行比较，确认请求是否过期。然后 CDN 服务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一致，访问合法，才会将资源返回给客户。



> #### 动态 CDN

**主要有两种模式**。

- 一种为**边缘计算的模式**。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。
- 另一种是**路径优化的模式**。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过 CDN 的网络，对路径进行优化。因为 CDN 节点较多，能够找到离源站很近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由 CDN  来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。

对于常用的 TCP 连接，在公网上传输的时候经常会丢数据，导致 TCP 的窗口始终很小，发送速度上不去。根据前面的 TCP 流量控制和拥塞控制的原理，在 CDN 加速网络中可以调整 TCP 的参数，使得 TCP 可以更加激进地传输数据。

可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或者建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。





# 请求全流程

![index](截图/计算机网络/请求流程图.jpg)

![浏览器输入url经历图](截图/计算机网络/浏览器输入url经历图.jpg)



1. 用户输入url，浏览器内部代码将url进行拆分解析
2. 浏览器首先去找本地的hosts文件，检查在该文件中是否有相应的域名、IP对应关系，如果有，则向其IP地址发送请求，如果没有就会将domain（域）发送给 dns进行解析，将域名解析成对应的服务器IP地址，发回给浏览器。
3. 浏览器拿到了服务器IP，接下来是网络通信，分层由高到低分别为：应用层、传输层、网络层、数据链路层。发送端从应用层往下走，接收端从数据链路层往上
4. 页面的渲染阶段
   1. 解析HTML生成DOM树。
   2. 解析CSS生成CSSOM规则树。
   3. 将DOM树与CSSOM规则树合并在一起生成渲染树。
   4. 遍历渲染树开始布局，计算每个节点的位置大小信息。
   5. 将渲染树每个节点绘制到屏幕。



[浏览器的一个请求从发送到返回都经历了什么？](https://www.cnblogs.com/echo-hui/p/9298203.html)



# IP命令

> #### 怎么查看 IP 地址？

在 Windows 上是 ipconfig，在 Linux 上是 ifconfig、 ip addr，也可以自行安装 net-tools 和 iproute2 这两个工具



> #### ifconfig 和 ip addr 的区别

> [【network】关于ifconfig与ip addr](https://www.jianshu.com/p/6fff29bd42b3)

net-tools(ifconfig、route、arp和netstat等命令行工具) 和 iproute2

- net-tools起源于BSD的TCP/IP工具箱，后来成为老版本Linux内核中配置网络功能的工具，但自2001年起，Linux社区已经对其停止维护。同时，一些Linux发行版比如Arch Linux和CentOS/RHEL 7则已经完全抛弃了net-tools，只支持iproute2。

作为网络配置工具的一份子，iproute2是linux下管理控制TCP/IP网络和流量控制的新一代工具包，旨在替代老派的工具链net-tools，即大家比较熟悉的ifconfig，arp，route，netstat等命令。

net-tools通过procfs(/proc)和ioctl系统调用去访问和改变内核网络配置，而iproute2则通过netlink套接字接口与内核通讯。

抛开性能而言，net-tools的用法给人的感觉是比较乱，而iproute2的用户接口相对net-tools来说相对来说，更加直观。比如，各种网络资源（如link、IP地址、路由和隧道等）均使用合适的对象抽象去定义，使得用户可使用一致的语法去管理不同的对象，更重要的是，到目前为止，iproute2仍处在[持续开发](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.kernel.org%2Fpub%2Flinux%2Futils%2Fnet%2Fiproute2%2F)中，所以，net-tools和iproute2都需要去学习掌握了。





![index](截图/计算机网络/net-tools与ipaddr.png)



运行一下 ip addr

```shell
root@test:~# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    	 valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
		   valid_lft forever preferred_lft forever

2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff
    inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fec7:7975/64 scope link 
       valid_lft forever preferred_lft forever
```



在 IP 地址的后面有个 scope，对于 eth0 这张网卡来讲，是 global，说明这张网卡是可以对外的，可以接收来自各个地方的包。对于 lo 来讲，是 host，说明这张网卡仅仅可以供本机相互通信。

lo 全称是**loopback**，又称**环回接口**，往往会被分配到 127.0.0.1 这个地址。这个地址用于本机通信，经过内核处理后直接返回，不会在任何网络中出现。

在 IP 地址的上一行是 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff，这个被称为**MAC 地址**。

<BROADCAST,MULTICAST,UP,LOWER_UP> 是干什么的？这个叫作**net_device flags**，**网络设备的状态标识**。

UP 表示网卡处于启动的状态；BROADCAST 表示这个网卡有广播地址，可以发送广播包；MULTICAST  表示网卡可以发送多播包；LOWER_UP 表示 L1 是启动的，也即网线插着呢。

最大传输单元  MTU 为 1500，这是以太网的默认值。MTU 是二层 MAC 层的概念。MAC 层有 MAC 的头，以太网规定连 MAC 头带正文合起来，不允许超过 1500 个字节。正文里面有 IP 的头、TCP 的头、HTTP 的头。如果放不下，就需要分片来传输。



qdisc pfifo_fast 

qdisc 全称是**queueing discipline**，中文叫**排队规则**。内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。

最简单的 qdisc 是 pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。pfifo_fast 稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。三个波段（band）的优先级也不相同。band 0 的优先级最高，band 2 的最低。如果 band 0 里面有数据包，系统就不会处理 band 1 里面的数据包，band 1 和 band 2 之间也是一样。数据包是按照服务类型（**Type of Service，TOS**）被分配到三个波段（band）里面的。TOS 是 IP 头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。





// todo 课后加餐的5篇文章里面有大量的问题，需要整理与归类



# 面试题

> ### TCP连接阶段

- ### 发送序号和确认序号问题

  例： TCP建立连接的过程采用三次握手，已知第三次握手报文的发送序列号为1000，确认序列号为2000，请问第二次握手报文的发送序列号和确认序列号分别为？
  答：看答案时请参考上面TCP连接建立的图。
  客户端：发送X
  服务端：发送Y， 确认X+1
  客户端：发送X+1（1000），确认Y+1（2000）
  可以反推第二次为1999,确认1000

  

- ### 为什么TCP建立连接要三次握手

  1. 首先得回答三次握手的目的是**同步连接双方的序列号**和**确认号**并**交换 TCP 窗口大小信息**。
  2. 回答为什么两次握手不行，C发送请求，S应答并分配资源，若S的应答没有到达C端，C认为连接未建立，而S认为建立了，S会在一段时间内保留分配的资源，如果大量C这样请求，S会崩溃 。
  3. 最后回答握手当然可以四次五次一直握下去，但三次已经够了，就没有必要了。总结下来一句话，主要目的防止在网络发生延迟或者丢包的情况下浪费资源。



- ### 为什么断开TCP连接需要进行四次握手 

  因为TCP连接是**全双工的网络协议**，允许同时通信的双方同时进行数据的收发，同样也允许收发两个方向的连接被独立关闭，以避免client数据发送完毕，向server发送FIN关闭连接，而server还有发送到client的数据没有发送完毕的情况。所以关闭TCP连接需要进行四次握手，每次关闭一个方向上的连接需要FIN和ACK两次握手。



-  ### tcp怎么保证有序传输的

  1. 主机每次发送数据时，TCP就给每个数据包分配一个序列号并且在一个特定的时间内等待接收主机对分配的这个序列号进行确认。
  2. 如果发送主机在一个特定时间内没有收到接收主机的确认，则发送主机会重传此数据包。接收主机利用序**列号**对接收的数据进行确认，以便检测对方发送的数据是否有丢失或者乱序等。
  3. 接收主机一旦收到已经顺序化的数据，它就将这些数据按正确的顺序重组成数据流并传递到高层进行处理。



- ### 讲下tcp的快速重传和拥塞机制

- ### 知不知道time_wait状态，这个状态出现在什么地方，有什么用？（参考quic）

  - 通信双方建立TCP连接后，主动关闭连接的一方就会进入TIME_WAIT状态。客户端主动关闭连接时，会发送最后一个ack后，然后会进入TIME_WAIT状态，再停留2个MSL时间(后有MSL的解释)，进入CLOSED状态。

  - TCP/IP协议就是这样设计的，是不可避免的。主要有两个原因:

    - 可靠地实现TCP全双工连接的终止

      TCP协议在关闭连接的四次握手过程中，最终的ACK是由主动关闭连接的一端（后面统称A端）发出的，如果这个ACK丢失，对方（后面统称B端）将重发出最终的FIN，因此A端必须维护状态信息（TIME_WAIT）允许它重发最终的ACK。如果A端不维持TIME_WAIT状态，而是处于CLOSED  状态，那么A端将响应RST分节，B端收到后将此分节解释成一个错误（在java中会抛出connection  reset的SocketException)。

      因而，要实现TCP全双工连接的正常终止，必须处理终止过程中四个分节任何一个分节的丢失情况，主动关闭连接的A端必须维持TIME_WAIT状态 。 

    - 允许老的重复分节在网络中消逝 

      TCP分节可能由于路由器异常而“迷途”，在迷途期间，TCP发送端可能因确认超时而重发这个分节，迷途的分节在路由器修复后也会被送到最终目的地，这个迟到的迷途分节到达时可能会引起问题。在关闭“前一个连接”之后，马上又重新建立起一个相同的IP和端口之间的“新连接”，“前一个连接”的迷途重复分组在“前一个连接”终止后到达，而被“新连接”收到了。为了避免这个情况，TCP协议不允许处于TIME_WAIT状态的连接启动一个新的可用连接，因为TIME_WAIT状态持续2MSL，就可以保证当成功建立一个新TCP连接的时候，来自旧连接重复分组已经在网络中消逝。



- ### 知道udp是不可靠的传输，如果你来设计一个基于udp差不多可靠的算法，怎么设计？



- ### 既然UDP是不可靠的，有什么方法可以使得UDP变成一个可靠的传输协议？

  



- ### http与https有啥区别？说下https解决了什么问题，怎么解决的？说下https的握手过程。

  - HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。
  - HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层（Secure Sockets Layer 安全套接层），HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。

  　　HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。



[TCP为什么要三次握手，不是两次四次？](http://blog.chinaunix.net/uid-20726927-id-2455485.html)

[为什么建立TCP连接需要三次握手，为什么断开TCP连接需要四次握手，TIME_WAIT状态的意义](https://www.cnblogs.com/zhoudayang/p/6012257.html)

[tcp窗口滑动以及拥塞控制](https://www.cnblogs.com/woaiyy/p/3554182.html)





> #### http 响应码 301 和 302 代表的是什么？有什么区别？

301 redirect: 301 代表永久性转移(Permanently Moved)。
302 redirect: 302 代表暂时性转移(Temporarily Moved )。 

[HTTP返回码中301与302的区别](https://blog.csdn.net/qmhball/article/details/7838989)



> #### 如何实现跨域？

[聊聊跨域的原理与解决方法](https://zhuanlan.zhihu.com/p/149734572)



> #### 说一下 JSONP 实现原理？

标签的src属性并不被同源策略所约束

[彻底弄懂jsonp原理及实现方法](https://www.cnblogs.com/soyxiaobi/p/9616011.html)